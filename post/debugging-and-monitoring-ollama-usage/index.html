<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Notes on learning deep learning and everything LLM-related">
    <title>Debugging and Monitoring Ollama Usage | Learning Deep Learning</title>
    <link rel="icon" href="/favicon.png" type="image/png">
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <style>
  body {
    background-image: radial-gradient(#8e8 1.2px, #eee 1.2px);
    background-size: 12px 12px;
  }

  main img {
    display: block;
    margin: 0 auto;
  }
</style>

  </head>

  <header style="text-align: center;">
    
      <a href="https://learningdeeplearning.com/">
        <img src="https://learningdeeplearning.com/images/logo.png" alt="Logo" style="max-width: 200px; height: auto; margin: 0 auto; display: block;">
      </a>
    
  </header>

  <body>
    <nav>
    <ul class="menu">
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Debugging and Monitoring Ollama Usage</span></h1>

<p class="date">Aug 12, 2024</p>
</div>

<main>
<p>Ollama is such a nice backend for running LLMs on your own, you can get it up and running in a single command, and
right after it&rsquo;s done, you can pull and chat with different models in no time.</p>
<p>While getting it up and running requires pretty much no configuration, debugging and monitoring it&rsquo;s not as obvious.
Here are some things I&rsquo;ve been doing myself on that regard.</p>
<h2 id="keep-an-eye-on-htop-and-nvtop">Keep an eye on htop and nvtop</h2>
<p>The first suggestion is pretty basic: keep an eye on CPU and GPU usage while Ollama is generating text. That will give
you an immediate answer if your GPU is being used or not. Ideally the GPU should be used to achieve much faster
inference times.</p>
<p><code>htop</code> will show CPU usage and <code>nvtop</code> (NVIDIA) GPU usage (or <code>radeontop</code> for AMD GPUs):</p>
<p><img src="/images/debugging-and-monitoring-ollama-usage/htop.png" alt=""></p>
<p><img src="/images/debugging-and-monitoring-ollama-usage/nvtop.png" alt=""></p>
<p><img src="/images/debugging-and-monitoring-ollama-usage/radeontop.png" alt=""></p>
<h2 id="get-familiar-with-eval-times-with---verbose">Get familiar with eval times with --verbose</h2>
<p>On top of checking top-like monitoring tools (no pun intended), you should get familiar with inference times of your
hardware.  To do that, run ollama passing <code>--verbose</code>. You will get the times spent to evaluate each prompt right after
each response:</p>
<pre tabindex="0"><code>&gt;&gt;&gt; what are the top 10 most famous literary authors
Sure, here are the top 10 most famous literary authors:

1. William Shakespeare
2. Homer
3. Dante Alighieri
4. Jane Austen
5. Victor Hugo
6. Mark Twain
7. J.K. Rowling
8. Ernest Hemingway
9. Charles Dickens
10. Emily Dickinson

total duration:       789.131073ms
load duration:        25.543401ms
prompt eval count:    38 token(s)
prompt eval duration: 18.123ms
prompt eval rate:     2096.78 tokens/s
eval count:           70 token(s)
eval duration:        703.692ms
eval rate:            99.48 tokens/s
</code></pre><p>I got values around ~100 tokens/s for a RTX 2060, ~75 tokens/s for RX 7600, and ~13 tokens/s for Ryzen 5 5500 (on tiny
models such as <code>gemma:2b</code>).</p>
<h2 id="check-the-modelfile-of-your-model">Check the modelfile of your model</h2>
<p>You can find crucial information about how a model is behaving by checking its <a href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md">Modelfile</a>,
settings such as its initial system prompt and a custom temperature can greatly affect how the model replies to prompts.</p>
<p>Sample command:</p>
<pre tabindex="0"><code>$ ollama show phi3 --modelfile
</code></pre><p>Sample output:</p>
<pre tabindex="0"><code># Modelfile generated by &quot;ollama show&quot;
# To build a new Modelfile based on this, replace FROM with:
# FROM phi3:latest

FROM /usr/share/ollama/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf
TEMPLATE &quot;{{ if .System }}&lt;|system|&gt;
{{ .System }}&lt;|end|&gt;
{{ end }}{{ if .Prompt }}&lt;|user|&gt;
{{ .Prompt }}&lt;|end|&gt;
{{ end }}&lt;|assistant|&gt;
{{ .Response }}&lt;|end|&gt;&quot;
PARAMETER stop &lt;|end|&gt;
PARAMETER stop &lt;|user|&gt;
PARAMETER stop &lt;|assistant|&gt;
LICENSE &quot;&quot;&quot;Microsoft.
Copyright (c) Microsoft Corporation.

MIT License

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the &quot;Software&quot;), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.&quot;&quot;&quot;
</code></pre><h2 id="theres-some-history-on-ollamahistory">There&rsquo;s some history on ~/.ollama/history</h2>
<p>You can find the history of prompts <code>~/.ollama/history</code>, but sadly it only contains the text you&rsquo;ve sent but not the
responses.</p>
<h2 id="enable-ollama_debug-and-check-the-service-logs">Enable OLLAMA_DEBUG and check the service logs</h2>
<p>It can be useful to have Ollama logging all prompts. While not as useful if you are typing out every single prompt
yourself on the CLI, but when connecting Ollama to other tools (tools performing RAG for instance) can be invaluable
seeing what&rsquo;s been injected on the prompts.</p>
<p>To have it logged, set the environment variable <code>OLLAMA_DEBUG</code> to <code>1</code>. If you want to do it directly on the systemd
service, edit <code>/etc/systemd/system/ollama.service</code> and add the following to the <code>[Service]</code> section:</p>
<pre tabindex="0"><code>Environment=&quot;OLLAMA_DEBUG=1&quot;
</code></pre><p>To check the logs:</p>
<pre tabindex="0"><code>journalctl -u ollama --no-pager
</code></pre><p>Here&rsquo;s what I got when prompting <a href="https://neilpostman.org/articles/Postman-TheEducationistAsPainkiller.pdf">this PDF article</a> while using <a href="https://openwebui.com/">OpenWebUI</a>:</p>
<pre tabindex="0"><code>&lt;|system|&gt;
Use the following context as your learned knowledge, inside &lt;context&gt;&lt;/context&gt; XML tags.
&lt;context&gt;
    the matter diligently. My own favorites include the Analects of Confucius and the early Dialogues of Plato, which are little else but meditations on stupidity. Acknowledging that he did not know what truth is, Socrates spent his time exposing the false beliefs of those who thought they did. I am also partial to Erasmus’ In Praise of Folly, Jonathan Swift’s Gulliver’s Travels, and, in a more modern vein, Jacques Ellul’s

and Ed.D. from Columbia University. He was the Paulette Goddard Chair of Media Ecology at New York University and chair of the Department of Culture and Communication. His scholarly interests included media, education, language, and technology.  For more information, visit www.neilpostman.org.

www.neilpostman.org 2 In modern times, the list of educationists continues to include formidable intellects—William James, for example, whose Talks to Teachers is among the best books on education ever written. Two of the greatest philosophers in this century, Ludwig Wittgenstein and Karl Popper, were elementary-school teachers who of necessity would have thought deeply about educational issues. Wittgenstein’s professor at Cambridge, Bertrand Russell, founded a school, and Russell’s colleague, Alfred North Whitehead, wrote the impeccable Aims of Education. And, of course, America’s greatest homegrown systematic philosopher, John Dewey, was an educa-tionist par excellence. In other words, the history of Western philosophy is so bound up with the subject of education that the two can hardly be separated. One might even say that just as it is natural for a physicist upon reaching his deepest understandings to be drawn toward religion, so it is natural for a mature philosopher to turn toward the problems of education. Why, then, this persistent prejudice against the subject and those who make a profession of its study? Definitive Answers await a rich and extensive research project to which sociologists, psychologists, historians, perhaps even anthropologists must contribute their perspectives. I mention anthropology because I suspect the intensity of the prejudice varies from culture to culture. There are places—China, for example—where the prejudice may not exist at all. But if

writing on education than Confucius and Plato, but he too was an educationist if we may take that word to mean a person who is seriously concerned to understand how learning takes place and what part schooling plays in facilitating or obstructing it. In this sense, Quintilian was an educationist, and so were Erasmus, John Locke, Rousseau, and Thomas Jefferson. The great English poet John Milton was so moved by the prospect of writing an essay on education that he called the reforming of education one of “the greatest and noblest designs to be thought on.”                                                    1 This article is copyright 1988 by the National Council of Teachers of English.  Reprinted with permission. “The Educationist as Painkiller” was originally published in English Education (1988), 7-17.  It was also published in Conscientious Objections (New York: Alfred A. Knopf, 1988), 82-96.    2 Postman, Neil. Conscientious Objections (New York: Alfred A. Knopf, 1988), 82.

The Educationist as Painkiller
&lt;/context&gt;

When answer to user:
- If you don't know, just say that you don't know.
- If you don't know when you are not sure, ask for clarification.
Avoid mentioning that you obtained the information from the context.
And answer according to the language of the user's question.

Given the context information, answer the query.
Query: give me a bullet list of the philosophers cited in the article&lt;|end|&gt;
&lt;|user|&gt;
give me a bullet list of the philosophers cited in the article&lt;|end|&gt;
&lt;|assistant|&gt;
</code></pre><h2 id="sources">Sources</h2>
<ul>
<li><a href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md">Ollama Model File</a></li>
<li><a href="https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md">How to troubleshoot issues</a></li>
<li><a href="https://neilpostman.org/#articles">All The Things You Never Even Knew You Wanted To Know About Neil Postman</a></li>
</ul>

</main>

    <hr>
    <footer>
      <p>
        <a href="mailto:talleslasmar@gmail.com">📧</a>
        | 
        <a href="https://github.com/tallesl"><img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" alt="GitHub" style="height: 25px; vertical-align: middle;"></a>
        |
        <a href="https://creativecommons.org/publicdomain/zero/1.0/"><img src="https://licensebuttons.net/p/zero/1.0/88x31.png" alt="CC0" style="height: 20px; vertical-align: middle;"></a>
      </p>
    </footer>
  </body>
</html>

