<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Notes on learning deep learning and everything LLM-related">
    <title>OWASP Top 10 LLM Vulnerabilities | Learning Deep Learning</title>
    <link rel="icon" href="/favicon.png" type="image/png">
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <style>
  body {
    background-image: radial-gradient(#8e8 1.2px, #eee 1.2px);
    background-size: 12px 12px;
  }

  main img {
    display: block;
    margin: 0 auto;
  }
</style>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-KEE68WFE95"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-KEE68WFE95');
</script>

  </head>

  <header style="text-align: center;">
    
      <a href="http://localhost:1313/">
        <img src="http://localhost:1313/images/logo.png" alt="Logo" style="max-width: 200px; height: auto; margin: 0 auto; display: block;">
      </a>
    
  </header>

  <body>
    <nav>
    <ul class="menu">
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">OWASP Top 10 LLM Vulnerabilities</span></h1>

<p class="date">Jul 29, 2024</p>
</div>

<main>
<p>The <a href="https://owasp.org/www-project-top-ten">OWASP Top 10</a> it&rsquo;s the go-to conversation starter about web security risks. With large language models grabbing all the attention nowadays, and while having their own set of security concerns, <a href="https://genai.owasp.org/llm-top-10/">OWASP rolled out a top 10 list just for LLMs</a>:</p>
<ul>
<li>#1 Prompt Injection</li>
<li>#2 Insecure Output Handling</li>
<li>#3 Training Data Poisoning</li>
<li>#4 Model Denial of Service</li>
<li>#5 Supply Chain Vulnerabilities</li>
<li>#6 Sensitive Information Disclosure</li>
<li>#7 Insecure Plugin Design</li>
<li>#8 Excessive Agency</li>
<li>#9 Overreliance</li>
<li>#10 Model Theft</li>
</ul>
<h2 id="1-prompt-injection">#1 Prompt Injection</h2>
<blockquote>
<p>&ldquo;(&hellip;) occurs when an attacker manipulates a large language model (LLM) through crafted inputs, causing the LLM to unknowingly execute the attacker‚Äôs intentions. This can be done directly by ‚Äújailbreaking‚Äù the system prompt or indirectly through manipulated external inputs (&hellip;).&rdquo;</p>
</blockquote>
<h2 id="2-insecure-output-handling">#2 Insecure Output Handling</h2>
<blockquote>
<p>&ldquo;An LLM plugin with open-ended functionality fails to properly filter the input instructions for commands outside what‚Äôs necessary for the intended operation of the application. E.g., a plugin to run one specific shell command fails to properly prevent other shell commands from being executed.&rdquo;</p>
</blockquote>

</main>

    <hr>
    <footer>
      <p>
        <a href="mailto:talleslasmar@gmail.com">üìß</a>
        | 
        <a href="https://github.com/tallesl"><img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" alt="GitHub" style="height: 25px; vertical-align: middle;"></a>
        |
        <a href="https://creativecommons.org/publicdomain/zero/1.0/"><img src="https://licensebuttons.net/p/zero/1.0/88x31.png" alt="CC0" style="height: 20px; vertical-align: middle;"></a>
      </p>
    </footer>
  </body>
</html>

