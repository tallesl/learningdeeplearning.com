<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Learning Deep Learning</title>
    <link>https://learningdeeplearning.com/post/</link>
    <description>Recent content in Posts on Learning Deep Learning</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Jul 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://learningdeeplearning.com/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Deep Learning on Arc A770 &#43; Ubuntu 22.04</title>
      <link>https://learningdeeplearning.com/post/deep-learning-on-arc-a770-ubuntu-22-04/</link>
      <pubDate>Fri, 19 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>https://learningdeeplearning.com/post/deep-learning-on-arc-a770-ubuntu-22-04/</guid>
      <description>The graphics processor consumer market is no longer dominated by the two-brand dispute of NVIDIA&amp;rsquo;s GeForce vs AMD&amp;rsquo;s Radeon. In 2022, a third contender entered the ring: Intel&amp;rsquo;s Arc.
It is still a relatively unknown option to most desktop enthusiasts, which may explain why it is priced lower than its competitors as of now. For folks like me, who are trying to get as much VRAM as possible on a budget, it&amp;rsquo;s simply hard to dismiss it.</description>
    </item>
    
    <item>
      <title>Deep Learning on RX 7600 &#43; Ubuntu 22.04</title>
      <link>https://learningdeeplearning.com/post/deep-learning-on-rx7600-ubuntu-22-04/</link>
      <pubDate>Wed, 03 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>https://learningdeeplearning.com/post/deep-learning-on-rx7600-ubuntu-22-04/</guid>
      <description>When it comes to deep learning, choosing the right hardware is important. It&amp;rsquo;s not ideal for the job, but I already had a budget-friendly RX 7600 video card from AMD. While AMD video cards can be challenging to set up with deep learning tools, I&amp;rsquo;m pleased to report that the situation is improving.
Note that the RX 7600 is not officially listed in the ROCm (Radeon Open Compute) support list. Despite this, I managed to get everything up and running smoothly on Ubuntu 22.</description>
    </item>
    
    <item>
      <title>Why GPUs Are Faster Than CPUs</title>
      <link>https://learningdeeplearning.com/post/why-gpus-are-faster-than-cpus/</link>
      <pubDate>Sat, 29 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://learningdeeplearning.com/post/why-gpus-are-faster-than-cpus/</guid>
      <description>They are not!
CPUs typically have higher clock speeds than GPUs. For instance, AMD&amp;rsquo;s Ryzen processors nearly reach 4 GHz, while NVIDIA&amp;rsquo;s GeForce 40 series barely reach 2.5 GHz. Although clock speed is a factor, it&amp;rsquo;s not always the most important aspect of a chip. Often, the capability to handle large volumes of computation simultaneously outweighs the processing speed.
The answer is straightforward:
GPUs excel in computing more data per tick of their clock in comparison to CPUs, thanks to their multiple cores and SIMD instructions.</description>
    </item>
    
    <item>
      <title>Safetensors vs GGUF</title>
      <link>https://learningdeeplearning.com/post/safetensors-vs-gguf/</link>
      <pubDate>Mon, 24 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://learningdeeplearning.com/post/safetensors-vs-gguf/</guid>
      <description>There are two popular formats found in the wild when getting a Llama 3 model: .safetensors and .gguf extension. Let&amp;rsquo;s get Llama 3 with both formats, analyze them, and perform inference on it (generate some text with it) using the most popular library for each format, covering:
 Getting Llama 3 from Meta website Converting .pth to .safetensors Safetensors origins Safetensors data format Safetensors inference (with HF&amp;rsquo;s transformers) Converting .safetensors to .</description>
    </item>
    
    <item>
      <title>A Heretical Theory No Longer</title>
      <link>https://learningdeeplearning.com/post/a-heretical-theory-no-longer/</link>
      <pubDate>Sun, 16 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://learningdeeplearning.com/post/a-heretical-theory-no-longer/</guid>
      <description>When talking about Artificial Intelligence, Turing&amp;rsquo;s name mostly comes due to its &amp;ldquo;Turing Test&amp;rdquo;, which is easily dismissed or even made fun of:
But Turing thoughts on thinking machines are not as superficial as it may seem at first glance.
&amp;ldquo;something very close to thinking&amp;rdquo; Back in 1951, Alan Turing gave a presentation arguing against the claim that &amp;ldquo;you cannot make a machine think for you&amp;rdquo;. Titled as &amp;ldquo;A Heretical Theory&amp;rdquo;, it&amp;rsquo;s interesting to see how things changed, what was heretical some decades ago is now what&amp;rsquo;s in vogue.</description>
    </item>
    
    <item>
      <title>Understanding Advance Vector Extensions (AVX)</title>
      <link>https://learningdeeplearning.com/post/understanding-advance-vector-extensions-avx/</link>
      <pubDate>Sat, 15 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://learningdeeplearning.com/post/understanding-advance-vector-extensions-avx/</guid>
      <description>I’ve been playing LLMs locally and an acronym that is a usual suspect on documentation pages is &amp;ldquo;AVX&amp;rdquo;.
Advanced Vector Extensions is a SIMD extension to x86 architecture. Well, that’s what Wikipedia says anyway.
Let’s get into this rabbit hole and figure out how AVX relates to LLMs.
Single Instruction, Multiple Data (SIMD) “Single Instruction, Multiple Data” is one of the best self-explanatory acronyms I’ve seen in a while! It&amp;rsquo;s pretty much what it says, performing a single instruction with a big set of registers:</description>
    </item>
    
    <item>
      <title>Running custom GGUF models with Ollama</title>
      <link>https://learningdeeplearning.com/post/running-custom-gguf-models-with-ollama/</link>
      <pubDate>Fri, 14 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://learningdeeplearning.com/post/running-custom-gguf-models-with-ollama/</guid>
      <description>Ollama is a convenient tool, with a single command it takes care of downloading, running, and putting you on a prompt:
$ ollama run llama3 pulling manifest pulling 6a0746a1ec1a... 100% ▕████████▏ 4.7 GB pulling 4fa551d4f938... 100% ▕████████▏ 12 KB pulling 8ab4849b038c... 100% ▕████████▏ 254 B pulling 577073ffcc6c... 100% ▕████████▏ 110 B pulling 3f8eb4da87fa... 100% ▕████████▏ 485 B verifying sha256 digest writing manifest removing any unused layers success &amp;gt;&amp;gt;&amp;gt; hello there Hello!</description>
    </item>
    
  </channel>
</rss>
