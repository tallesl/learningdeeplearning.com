<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Notes on learning deep learning and everything LLM-related">
    <title>Computing Logic Gates with Perceptrons | Learning Deep Learning</title>
    <link rel="icon" href="/favicon.png" type="image/png">
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <style>
  body {
    background-image: radial-gradient(#8e8 1.2px, #eee 1.2px);
    background-size: 12px 12px;
  }

  main img {
    display: block;
    margin: 0 auto;
  }
</style>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-KEE68WFE95"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-KEE68WFE95');
</script>

  </head>

  <header style="text-align: center;">
    
      <a href="https://learningdeeplearning.com/">
        <img src="https://learningdeeplearning.com/images/logo.png" alt="Logo" style="max-width: 200px; height: auto; margin: 0 auto; display: block;">
      </a>
    
  </header>

  <body>
    <nav>
    <ul class="menu">
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Computing Logic Gates with Perceptrons</span></h1>

<p class="date">Aug 30, 2024</p>
</div>

<main>
<p>This article explores the perceptron algorithm, focusing on how it computes rather than how it learns. We will start by
briefly mentioning its creator, followed by a close examination how the algorithm works. To conclude, we will add a
hidden layer and connect the neurons making a network, that will then implement the non-linear XOR gate.</p>
<h2 id="an-idea-from-the-1950s">An idea from the 1950s</h2>
<p>Frank Rosenblatt developed the Perceptron in 1958, an early artificial neuron model that mimicked the computational
processes of biological neurons.</p>
<p><img src="/images/computing-logic-gates-with-perceptrons/rosenblatt-nyt.png" alt=""></p>
<p>While Rosenblatt was undeniably a visionary, his expectations were somewhat inflated. The promises he made are only now
beginning to be fulfilled.</p>
<h2 id="the-perceptron-algorithm">The perceptron algorithm</h2>
<p><img src="/images/computing-logic-gates-with-perceptrons/perceptron-diagram.png" alt=""></p>
<p>The perceptron works like this:</p>
<ul>
<li>Takes ùëõ inputs of the same size, which can be scalar values or vectors (x‚ÇÅ, x‚ÇÇ, x‚ÇÉ)</li>
<li>Multiply each input by its own weight, a floating point number (w‚ÇÅ, w‚ÇÇ, w‚ÇÉ)</li>
<li>Sum the weighted inputs ( ‚àë )</li>
<li>Add a bias term to shift the function, sliding its line to either the left or right on a graph (b)</li>
<li>Pass the sum through the step function to get the final output (step)</li>
</ul>
<p>Which can be viewed as:</p>
<pre tabindex="0"><code>output = step(dot(x_values, w_values) + b)
</code></pre><p>Let&rsquo;s understand each individual component of this computation below.</p>
<h2 id="dot-product">Dot product</h2>
<p>For the weighted summation (simbolized by ‚àë on the diagram), we should do:</p>
<pre tabindex="0"><code>(x‚ÇÅ * w‚ÇÅ) + (x‚ÇÇ * w‚ÇÇ) + (x‚ÇÉ * w‚ÇÉ)
</code></pre><p>Let&rsquo;s take the following values as an example:</p>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>x‚ÇÅ</td>
<td>1</td>
</tr>
<tr>
<td>x‚ÇÇ</td>
<td>2</td>
</tr>
<tr>
<td>x‚ÇÉ</td>
<td>3</td>
</tr>
<tr>
<td>w‚ÇÅ</td>
<td>0.1</td>
</tr>
<tr>
<td>w‚ÇÇ</td>
<td>0.2</td>
</tr>
<tr>
<td>w‚ÇÉ</td>
<td>0.3</td>
</tr>
</tbody>
</table>
<p>This results in:</p>
<pre tabindex="0"><code>(1 * 0.1) + (2 * 0.2) + (3 * 0.3) =
0.1 + 0.4 + 0.9
1.4
</code></pre><p>Since x and w are of equal length, we can use the mathematical dot product operation to perform the summation we just
saw above. Dot product takes two equal-length sequences of numbers and returns a single number:</p>
<p><img src="/images/computing-logic-gates-with-perceptrons/dotproduct.png" alt=""></p>
<h2 id="bias">Bias</h2>
<p>A common technique to shift a function curve left or right is to add a (constant) value to its input, known as a
<strong>&ldquo;bias&rdquo;</strong>.</p>
<p>To illustrate, consider the sigmoid function (in blue). Normally, for x = 0, y = 1. By adding a bias of -1, we shift the
function to the right, now for x = 0, y = 0.27 (which is just sigmoid(-1)).</p>
<p>Don&rsquo;t worry about the sigmoid function for now, it will be introduced later on this article.</p>
<p><img src="/images/computing-logic-gates-with-perceptrons/sigmoid-bias.png" alt=""></p>
<h2 id="step-function">Step function</h2>
<p>After computing the weighted sum, we pass the values to the step function:</p>
<ul>
<li>for x &lt; 0, y = 0</li>
<li>for x &gt;= 0, y = 1</li>
</ul>
<p>It essentially works as an on/off switch, off when negative and on when positive.</p>
<p><img src="/images/computing-logic-gates-with-perceptrons/step.png" alt=""></p>
<h2 id="not-gate">NOT gate</h2>
<table>
<thead>
<tr>
<th>a</th>
<th>NOT a</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>The NOT gate that inverts the value.</p>
<p>Using the perceptron algorithm we discussed earlier (weighted sum, bias, and step function), we can implement the NOT
gate with a single weight and bias.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">dot</span>(vector1, vector2):
    <span style="color:#66d9ef">return</span> sum(x <span style="color:#f92672">*</span> y <span style="color:#66d9ef">for</span> x, y <span style="color:#f92672">in</span> zip(vector1, vector2))

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">step</span>(x):
    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span> <span style="color:#66d9ef">if</span> x <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_perceptron</span>(weights, bias, input):
    <span style="color:#66d9ef">return</span> step(dot(weights, input) <span style="color:#f92672">+</span> bias)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_not</span>(value):
    not_weights <span style="color:#f92672">=</span> [<span style="color:#f92672">-</span><span style="color:#ae81ff">2.</span>]
    not_bias <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.</span>
    <span style="color:#66d9ef">return</span> compute_perceptron(not_weights, not_bias, [value])

print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;NOT 0: </span><span style="color:#e6db74">{</span>compute_not(<span style="color:#ae81ff">0</span>)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>) <span style="color:#75715e"># prints NOT 0: 1</span>
print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;NOT 1: </span><span style="color:#e6db74">{</span>compute_not(<span style="color:#ae81ff">1</span>)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>) <span style="color:#75715e"># prints NOT 1: 0</span>
</code></pre></div><h2 id="and-and-or-gates">AND and OR gates</h2>
<table>
<thead>
<tr>
<th>a</th>
<th>b</th>
<th>a AND b</th>
<th>a OR b</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>Using a couple of weights and a bias, we can compute AND and OR gates.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">dot</span>(vector1, vector2):
    <span style="color:#66d9ef">return</span> sum(x <span style="color:#f92672">*</span> y <span style="color:#66d9ef">for</span> x, y <span style="color:#f92672">in</span> zip(vector1, vector2))

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">step</span>(x):
    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span> <span style="color:#66d9ef">if</span> x <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_perceptron</span>(weights, bias, input):
    <span style="color:#66d9ef">return</span> step(dot(weights, input) <span style="color:#f92672">+</span> bias)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_and</span>(value_a, value_b):
    and_weights <span style="color:#f92672">=</span> [<span style="color:#ae81ff">2.</span>, <span style="color:#ae81ff">2.</span>]
    and_bias <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">3.</span>
    <span style="color:#66d9ef">return</span> compute_perceptron(and_weights, and_bias, [value_a, value_b])

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_or</span>(value_a, value_b):
    or_weights <span style="color:#f92672">=</span> [<span style="color:#ae81ff">2.</span>, <span style="color:#ae81ff">2.</span>]
    or_bias <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>
    <span style="color:#66d9ef">return</span> compute_perceptron(or_weights, or_bias, [value_a, value_b])

print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;0 AND 0: </span><span style="color:#e6db74">{</span>compute_and(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>) <span style="color:#75715e"># prints 0 AND 0: 0</span>
print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;0 AND 1: </span><span style="color:#e6db74">{</span>compute_and(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>) <span style="color:#75715e"># prints 0 AND 1: 0</span>
print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;1 AND 0: </span><span style="color:#e6db74">{</span>compute_and(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>) <span style="color:#75715e"># prints 1 AND 0: 0</span>
print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;1 AND 1: </span><span style="color:#e6db74">{</span>compute_and(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>) <span style="color:#75715e"># prints 1 AND 1: 1</span>
print()
print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;0 OR 0: </span><span style="color:#e6db74">{</span>compute_or(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>) <span style="color:#75715e"># prints 0 OR 0: 0</span>
print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;0 OR 1: </span><span style="color:#e6db74">{</span>compute_or(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>) <span style="color:#75715e"># prints 0 OR 1: 1</span>
print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;1 OR 0: </span><span style="color:#e6db74">{</span>compute_or(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>) <span style="color:#75715e"># prints 1 OR 0: 1</span>
print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;1 OR 1: </span><span style="color:#e6db74">{</span>compute_or(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>) <span style="color:#75715e"># prints 1 OR 1: 1</span>
</code></pre></div><h2 id="xor-gate">XOR gate</h2>
<table>
<thead>
<tr>
<th>a</th>
<th>b</th>
<th>a XOR b</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>Unlike the other logic gates, the XOR gate is not linearly separable, that is, we cannot draw a single line to separate
the true values from the false. As a result, a single perceptron cannot implement the XOR gate, a challenge known as
<strong>&ldquo;the XOR problem&rdquo;</strong>.</p>
<p>Notice how the blue and red dots on the graph below cannot be separated by a single line.</p>
<p><img src="/images/computing-logic-gates-with-perceptrons/xor.png" alt=""></p>
<h2 id="neural-network">Neural network</h2>
<p>A single perceptron cannot compute the XOR gate because it is a non-linear function. However, by using more than one
neuron and stacking them in layers, we can create what is called an <strong>&ldquo;artificial neural network&rdquo;</strong>.</p>
<p>Below is the structure of the neural network that we will implement in this article.</p>
<p><img src="/images/computing-logic-gates-with-perceptrons/network.png" alt=""></p>
<p>A typical neural network consists of an input layer, followed by one or more hidden layers, and an output layer.</p>
<p>In the neural network we are building to compute the XOR logic gate, the input layer has two neurons corresponding to
the two inputs of the XOR gate, and the output layer has one neuron, reflecting the single output of the gate. The
hidden layer contains two neurons, which is the minimum number required to solve the XOR problem.</p>
<p>Neural networks can have hidden layers of varying sizes. When a network has multiple hidden layers, it is referred to as
a <strong>&ldquo;deep&rdquo;</strong> network. In this <strong>&ldquo;feed-forward&rdquo;</strong> network, the input propagates in a single direction, from the input
layer to the output layer. Additionally, because each neuron in one layer is connected to every neuron in the subsequent
layer, the network is termed <strong>&ldquo;fully connected&rdquo;</strong> or <strong>&ldquo;dense&rdquo;</strong>.</p>
<p>Each neuron typically includes an adjustable bias term (added to the summation of inputs), which is essential for
shifting the activation function. However, for simplicity, biases are omitted from this diagram.</p>
<h2 id="sigmoid-function">Sigmoid function</h2>
<p>Before tackling the XOR problem, we are introducing a key change: instead of using the step function after calculating
the weighted sum of inputs and biases, we will use the sigmoid function.</p>
<p>Observe in the plot below that both the sigmoid and step functions transition from 0 to 1. However, the key difference
is that the sigmoid function provides a smooth, gradual transition, unlike the abrupt change seen in the step function.</p>
<p>The sigmoid function is an example of an <strong>&ldquo;activation function&rdquo;</strong>, which determines whether or not a neuron is
activated based on its output.</p>
<p><img src="/images/computing-logic-gates-with-perceptrons/sigmoid-bias.png" alt=""></p>
<h2 id="solving-the-xor-problem">Solving the XOR problem</h2>
<p>Lastly, let&rsquo;s build our neural network and manually set the weights that we know in advance will allow the network to
solve the XOR problem.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">from</span> math <span style="color:#f92672">import</span> exp

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">dot</span>(vector1, vector2):
    <span style="color:#66d9ef">return</span> sum(x <span style="color:#f92672">*</span> y <span style="color:#66d9ef">for</span> x, y <span style="color:#f92672">in</span> zip(vector1, vector2))

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sigmoid</span>(x):
    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> exp(<span style="color:#f92672">-</span>x))

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_neuron</span>(weights, inputs):
    bias <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span>] <span style="color:#75715e"># for this example we&#39;ll use a constant bias of 1</span>
    input_with_bias <span style="color:#f92672">=</span> inputs <span style="color:#f92672">+</span> bias <span style="color:#75715e"># making a list with inputs and bias</span>
    <span style="color:#66d9ef">return</span> sigmoid(dot(weights, input_with_bias))

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_network</span>(layers, input):
    outputs <span style="color:#f92672">=</span> []

    <span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> layers:
        output <span style="color:#f92672">=</span> [compute_neuron(neuron_weights, input) <span style="color:#66d9ef">for</span> neuron_weights <span style="color:#f92672">in</span> layer]
        outputs<span style="color:#f92672">.</span>append(output)
        input <span style="color:#f92672">=</span> output

    <span style="color:#66d9ef">return</span> outputs

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_xor</span>(value_a, value_b):
    layers <span style="color:#f92672">=</span> [

        <span style="color:#75715e"># hidden layer</span>
        [[<span style="color:#ae81ff">20.</span>, <span style="color:#ae81ff">20.</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">30</span>], [<span style="color:#ae81ff">20.</span>, <span style="color:#ae81ff">20.</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">10.</span>]],

        <span style="color:#75715e"># output layer</span>
        [[<span style="color:#f92672">-</span><span style="color:#ae81ff">60.</span>, <span style="color:#ae81ff">60.</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">30.</span>]]

    ]

    outputs <span style="color:#f92672">=</span> compute_network(layers, [value_a, value_b])
    last_output <span style="color:#f92672">=</span> outputs[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">0</span>]

    <span style="color:#66d9ef">return</span> round(last_output)

print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;0 XOR 0: </span><span style="color:#e6db74">{</span>compute_xor(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>) <span style="color:#75715e"># prints 0 XOR 0: 0</span>
print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;0 XOR 1: </span><span style="color:#e6db74">{</span>compute_xor(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>) <span style="color:#75715e"># prints 0 XOR 1: 1</span>
print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;1 XOR 0: </span><span style="color:#e6db74">{</span>compute_xor(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>) <span style="color:#75715e"># prints 1 XOR 0: 1</span>
print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;1 XOR 1: </span><span style="color:#e6db74">{</span>compute_xor(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>) <span style="color:#75715e"># prints 1 XOR 1: 0</span>
</code></pre></div><p>We‚Äôve successfully built a neural network to solve the XOR problem, demonstrating how adding layers and using the right
activation functions can tackle non-linear challenges.</p>
<p>This shows how even simple neural networks can handle tricky problems, paving the way for more complex applications.</p>
<h2 id="sources">Sources</h2>
<ul>
<li>Weights values taken from <a href="https://www.google.com/books/edition/Data_Science_from_Scratch/JYodCAAAQBAJ">&ldquo;Data Science from Scratch&rdquo;</a></li>
<li><a href="https://www.nytimes.com/1958/07/13/archives/electronic-brain-teaches-itself.html">Electronic &lsquo;Brain&rsquo; Teaches Itself (1958)</a></li>
</ul>

</main>

    <hr>
    <footer>
      <p>
        <a href="mailto:talleslasmar@gmail.com">üìß</a>
        | 
        <a href="https://github.com/tallesl"><img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" alt="GitHub" style="height: 25px; vertical-align: middle;"></a>
        |
        <a href="https://creativecommons.org/publicdomain/zero/1.0/"><img src="https://licensebuttons.net/p/zero/1.0/88x31.png" alt="CC0" style="height: 20px; vertical-align: middle;"></a>
      </p>
    </footer>
  </body>
</html>

